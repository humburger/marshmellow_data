I could read data only from database backups which where >2GB by size. Plain working with SQL reading tool was too slow, so I created 'drop_create.py' to make it simpler.

Then to gain necessary data use 'species' where needs to precise SQL code for necessary data results. And then code using other scripts will create excel file or multiple files, depending how large data there is. I mostly try to limit only 10000 data per excel file.
