I could read data only from database backups which where >2GB by size. Plain working with SQL reading tool was too slow, so I created 'drop_create.py' to make it simpler.

Then to gain necessary data use 'species' where needs to specify SQL code for necessary data results. And then using other scripts excel file or multiple files will be created, depending how large data there is. I mostly try to limit only 10000 data per excel file.
